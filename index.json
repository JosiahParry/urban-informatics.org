[{"categories":["tutorial"],"contents":"In the last tutorial we went over the concept of spatial autocorrelation and Moran\u0026rsquo;s I. Moran\u0026rsquo;s I is a global measure meaning it characterizes the entire dataset. It is helpful in informing us that there is or is not some degree of spatialness to our data. But it doesn\u0026rsquo;t help us understand where clustering might occur. For that, we need to move our analysis down to a more local level.\nAlternative to Moran\u0026rsquo;s I measure of global autocorrelation, we can use local indicators of spatial association (LISA). Moran\u0026rsquo;s I looks at all units collectively whereas LISA calculates an I for each group of neighbors. If Moran\u0026rsquo;s I does not indicate a global autocorrelation, that does not rule out the possibility of any local clustering which can be sussed out with LISA.\nGetting started For this tutorial we will use the same dataset as previously. Below is the code used to create the sf object. Note that if you haven\u0026rsquo;t yet installed {sfweight} you will need to do so with remotes::install_github(\u0026quot;josiahparry/sfweight\u0026quot;).\nlibrary(sf) library(sfweight) library(tidyverse) acs \u0026lt;- select(uitk::acs_raw, fips = ct_id_10, med_house_income, by_pub_trans, bach) %\u0026gt;% mutate(fips = as.character(fips), across(.cols = c(med_house_income, by_pub_trans, bach), ~replace_na(.x, median(.x, na.rm = TRUE)))) acs_sf \u0026lt;- left_join(uitk::suffolk_county, acs, by = \"fips\")  In order to calculate local measures of spatial autocorrelation, we will need three things:\n neighbors of each observation spatial weights spatial lag  We\u0026rsquo;ve previously covered the first two items, but not the third. Before we jump into spatially lagged variables, let\u0026rsquo;s create the neighbors and weights and store them in an object acs_nb.\nacs_nb \u0026lt;- acs_sf %\u0026gt;% mutate(nb = st_neighbors(geometry), wt = st_weights(nb))  The spatial lag With the neighbors and weight matrix calculated, we can begin the process of calculating our LISA. In order to do so we must calculate the spatial lag. The spatial lag takes our variable of interest and averages it for a location\u0026rsquo;s neighbors. By taking the average we are accounting for neighborhood effects and this local average can then be compared with the locations actual value to see if there is a discrepancy between itself and its neighbors.\nLet\u0026rsquo;s look at the first observation in our acs_nb object to see how this is calculated. First we\u0026rsquo;ll take the neighbor indexes from the nb list column. Then we\u0026rsquo;ll use that to grab those rows and the original observation itself using slice().\n# grab neighbors nb_index \u0026lt;- slice(acs_nb, 1) %\u0026gt;% pull(nb) %\u0026gt;% pluck(1) # slice to only neighbor rows and original row nb_1 \u0026lt;- acs_nb %\u0026gt;% slice(1, nb_index) %\u0026gt;% select(bach) # plot bach ggplot(nb_1, aes(fill = bach)) + geom_sf(lwd = 0.2, color = \"black\") + scale_fill_gradient(high = \"#528672\")   We can see that the middle census tract may has quite a low level of educational attainment when compared to the tract above and to it\u0026rsquo;s lower right. To calculate the spatial lag of this census tract, we need to take the average of all observations. Their values are as follows\nnb_1 #\u0026gt; Simple feature collection with 8 features and 1 field #\u0026gt; Geometry type: MULTIPOLYGON #\u0026gt; Dimension: XY #\u0026gt; Bounding box: xmin: -71.07854 ymin: 42.28378 xmax: -71.03529 ymax: 42.32171 #\u0026gt; Geodetic CRS: WGS 84 #\u0026gt; # A tibble: 8 x 2 #\u0026gt; bach geometry #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;MULTIPOLYGON [Â°]\u0026gt; #\u0026gt; 1 0.124 (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29301, -71.0621â€¦ #\u0026gt; 2 0.305 (((-71.05147 42.28931, -71.05136 42.28933, -71.05032 42.28961, -71.0484â€¦ #\u0026gt; 3 0.363 (((-71.06127 42.30907, -71.06103 42.30961, -71.061 42.30968, -71.06083 â€¦ #\u0026gt; 4 0.141 (((-71.06649 42.30671, -71.06584 42.30731, -71.06551 42.30762, -71.0650â€¦ #\u0026gt; 5 0.103 (((-71.07364 42.29937, -71.07293 42.29958, -71.07274 42.29966, -71.0726â€¦ #\u0026gt; 6 0.198 (((-71.07212 42.29551, -71.07133 42.29568, -71.07115 42.29573, -71.0702â€¦ #\u0026gt; 7 0.126 (((-71.07851 42.28943, -71.0784 42.28981, -71.07831 42.29017, -71.07826â€¦ #\u0026gt; 8 0.212 (((-71.06373 42.28778, -71.06364 42.28811, -71.06351 42.28855, -71.0634â€¦  To calculate the average we can pull out the bach vector, excluding the observation itself, and take the mean.\nnb_1 %\u0026gt;% slice(-1) %\u0026gt;% pull(bach) %\u0026gt;% mean() #\u0026gt; [1] 0.2069233  The spatial lag for that observation is 0.206 This makes sense as it\u0026rsquo;s neighbors had clearly higher values. Rather than do this for each and every observation we can utilize the function st_lag() which takes 3 arguments:\n x: the numeric variable of interest, neighbors: the list of neighbors weights: the list of weights to be used.  Let\u0026rsquo;s calculate the lag for bach and store it in variable bach_lag.\nacs_lag \u0026lt;- acs_nb %\u0026gt;% mutate(bach_lag = st_lag(bach, nb, wt)) acs_lag %\u0026gt;% select(bach, bach_lag) #\u0026gt; Simple feature collection with 203 features and 2 fields #\u0026gt; Geometry type: MULTIPOLYGON #\u0026gt; Dimension: XY #\u0026gt; Bounding box: xmin: -71.19125 ymin: 42.22793 xmax: -70.9201 ymax: 42.45012 #\u0026gt; Geodetic CRS: WGS 84 #\u0026gt; # A tibble: 203 x 3 #\u0026gt; bach bach_lag geometry #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;MULTIPOLYGON [Â°]\u0026gt; #\u0026gt; 1 0.124 0.207 (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29301â€¦ #\u0026gt; 2 0.305 0.208 (((-71.05147 42.28931, -71.05136 42.28933, -71.05032 42.28961â€¦ #\u0026gt; 3 0.405 0.381 (((-71.11093 42.35047, -71.11093 42.3505, -71.11092 42.35054,â€¦ #\u0026gt; 4 0.141 0.289 (((-71.06944 42.346, -71.0691 42.34661, -71.06884 42.3471, -7â€¦ #\u0026gt; 5 0.208 0.264 (((-71.13397 42.25431, -71.13353 42.25476, -71.13274 42.25561â€¦ #\u0026gt; 6 0.365 0.428 (((-71.04707 42.3397, -71.04628 42.34037, -71.0449 42.34153, â€¦ #\u0026gt; 7 0.159 0.179 (((-71.01324 42.38301, -71.01231 42.38371, -71.01162 42.3842,â€¦ #\u0026gt; 8 0.230 0.159 (((-71.00113 42.3871, -71.001 42.38722, -71.00074 42.3875, -7â€¦ #\u0026gt; 9 0.266 0.296 (((-71.05079 42.32083, -71.0506 42.32076, -71.05047 42.32079,â€¦ #\u0026gt; 10 0.199 0.184 (((-71.11952 42.28648, -71.11949 42.2878, -71.11949 42.28792,â€¦ #\u0026gt; # â€¦ with 193 more rows  We can see that the lag is the same for the first observation as what we calculated by hand.\nWith this new lagged variable we can create a scatterplot that compares the original variable to the lagged variable. This is called a Moran plot. It helps us identify observations that are similar or dissimilar from their neighbors.\nacs_lag %\u0026gt;% ggplot(aes(bach, bach_lag)) + geom_point(alpha = 3/4)   There are typically a number of enhancements to this plot that are made to make it easier to comprehend. The observations are grouped by quadrant where the I is high-high (HH), II is high-low (HL), III is low-low (LL), and IV is low-high (LH). Observations that fall in the category of HH and LL indicate local clusters given that high values are around high values and low by low. The II and IV quadrants are what may be the most interesting, though. These are typically observations that might have a stark difference with their adjacent neighbors.\nTo improve this chart we can categorize the points and add lines indicating the means of x and y. For categorization we can use the categorize_lisa() function which adds the groups. To add the lines we will use geom_vline() and geom_hline() respectively.\nacs_lag %\u0026gt;% mutate(lisa_cat = categorize_lisa(bach, bach_lag)) %\u0026gt;% ggplot(aes(bach, bach_lag, color = lisa_cat)) + geom_point(alpha = 3/4) + geom_vline(aes(xintercept = mean(bach)), lty = 2) + geom_hline(aes(yintercept = mean(bach_lag)), lty = 2) + labs(title = \"Edu. attainment Moran plot\", y = \"Lagged Edu. attainment rate\", x = \"Edu. attainment rate\", color = \"Category\")   Now that we have the intuition of the spatial lag and its relationship with the local clusters, we can run the LISA.\nLISA In sfweight, LISAs are created with local_moran(). local_moran() creates a dataframe column that we can extract with unpack() from the tidyr package. Unfortunately unpack() doesn\u0026rsquo;t work with sf objects so we\u0026rsquo;ll need to cast acs_lag to a tibble with as_tibble(), then unpack(), then back to sf with st_as_sf().\nacs_lisa \u0026lt;- acs_lag %\u0026gt;% mutate(lisa = local_moran(bach, nb, wt)) %\u0026gt;% as_tibble() %\u0026gt;% unpack(lisa) %\u0026gt;% st_as_sf()  The resulting columns provide the local I, expected I, variance, z-value, and p value of each observation.\nacs_lisa %\u0026gt;% as_tibble() %\u0026gt;% select(contains(\"ii\")) #\u0026gt; # A tibble: 203 x 5 #\u0026gt; ii e_ii var_ii z_ii p_ii #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 0.315 -0.00495 0.138 0.863 0.194  #\u0026gt; 2 -0.137 -0.00495 0.328 -0.231 0.592  #\u0026gt; 3 1.22 -0.00495 0.245 2.47 0.00678 #\u0026gt; 4 -0.253 -0.00495 0.161 -0.618 0.732  #\u0026gt; 5 -0.0363 -0.00495 0.245 -0.0634 0.525  #\u0026gt; 6 1.23 -0.00495 0.245 2.49 0.00637 #\u0026gt; 7 0.379 -0.00495 0.138 1.03 0.150  #\u0026gt; 8 0.103 -0.00495 0.195 0.245 0.403  #\u0026gt; 9 0.0474 -0.00495 0.494 0.0745 0.470  #\u0026gt; 10 0.195 -0.00495 0.245 0.405 0.343  #\u0026gt; # â€¦ with 193 more rows  With this information, we can now identify where any significant clusters may be. We can do this by creating a column that classifies the LISA only if the p value is less than some threshold (alpha = 0.1, in this case). Then we can plot those classifications on our map of Boston.\nacs_lisa %\u0026gt;% mutate(sig_clusters = ifelse( # conditional statement p_ii \u0026lt;= 0.1, # when true categorize_lisa(bach, bach_lag), #when false NA)) %\u0026gt;% ggplot(aes(fill = sig_clusters)) + geom_sf(color = \"black\", lwd = 0.2) + scale_fill_manual(values = c(\"#528672\", \"#525586\"), na.translate = FALSE) + labs(title = \"Significant local clusters\", fill = \"Category\")   This map shows us that there are three distinct local clusters. The clusters, one near Chelsea and Revere and the other containing (as far as I can guess by the shape!) Roxbury, Dorchester, and Mattapan indicate that these areas have significantly lower educational attainment rates than average. Additionally, the areas of Downtown, South Boston, and Allston all have a higher level of educational attainment.\n","permalink":"/tutorials/2021-06-22-lisa-what-s-spatial-lag/","tags":["sfweight","rspatial"],"title":"LISA, what's spatial lag?"},{"categories":[],"contents":"Urban Informatics is a field of study based strictly on the urban environment and more specifically, cities. The urban environment is a very real and physical place. It is not some metaphysical conception that can only be spoken about in broad strokes. There are buildings, roads and intersections, land marks, restaurants and many other physical attributes that make up a city.\nThe city is a place. But when we refer to the city is a place, we are referring to some amorphous geographical boundary which has within it, more places. Boston is a city with clear physical boundaries on it\u0026rsquo;s Eastern and Northern sides\u0026mdash;the Atlantic and the Charles river\u0026mdash;though within it there are further places. There are neighborhoods like the South End and Back Bay which are also places with clearer and distinct boundaries. Within these there are further places like Blackstone Square and the Prudential Centers. These smaller places server a general interest. They can be referred to as land marks and are referenceable with a large degree of accuracy. These types of places are called places of interest.\nPlaces of interest, or POI, are locations that can be accurately located on the map which have some interest to the general public. POI have become increasingly important to the field of urban informatics with companies such as SafeGraph and Foursquare providing place based data. The data can be used to understand land use patterns, street networks, and so much more. Many people are using POI data extensively to understand the coronavirus pandemic (see SafeGraph research here).\nOne of the largest challenges to POI data is that of coverage. How many of the total POIs in a region are actually recorded in a database? How can we be certain that when using POI data that it is accurate? How do we best record changes to the environment when a POI, say restaurant, now longer exists?\nFrom my perspective, there seems to be no cohesive or theoretical definition of a place of interest other than \u0026ldquo;a place of interest.\u0026rdquo; Perhaps the phrase is so simple that it does not need further clarification. But there are still some questions that I suspect need further elaboration. Most pressingly, and this may be a futile line of thought, what actually makes a POI interesting? Seemingly anything maybe a POI and is determined by those who generate or provide the data. The criteria seems that the data must 1) have a coordinate point and 2) be able to described as a place.\nThe Boston Area Research Initiative (BARI) collects Boston based POI data. Among the many data resources they provide are the locations of Boston Airbnbs as provided by Inside Airbnb. These data are used extensively within the Urban Informatics program.\nThe below map utilizes the Airbnb POI data to illustrate spatial distribution and price of Airbnb listings within Boston.\n ","permalink":"/blog/2021-04-26-understanding-places-of-interest/","tags":["POI"],"title":"Understanding Places of Interest"},{"categories":["tutorial"],"contents":"Overview Tobler\u0026rsquo;s first law of geography states that\n \u0026ldquo;everything is related to everything else, but near things are more related than distant things.\u0026rdquo; - Waldo Tobler (source)\n In regular statistical analyses, we look at the relationship between two variables irrespective of their place in space. If there is a desire to account for space, this will likely be done by creating groups or regional identifiers that can be controlled for in a linear regression. For example, neighborhood or state level dummy variables or IDs.\nWe can move beyond this naive approach to incorporating space into our analysis to something a bit more explicit. We can check to see if there are measurable spatial relationships in our dataset as opposed to strictly measuring numeric correlation.\nThis post will introduce the concept of spatial autocorrelation. First we will create a dataset to work with. Then we\u0026rsquo;ll review neighbors, weights, and autocorrelation.\nCreating our dataset In this analysis, we will explore the spatial relationship of bachelor\u0026rsquo;s degree attainment in Suffolk County (Greater Boston). The data we will be using comes from the Urban Informatics Toolkit\u0026rsquo;s associate package {uitk}. This can be installed with remotes::install_github(\u0026quot;josiahparry/uitk\u0026quot;).\nThe package exports acs_raw which is a tibble containing socio-economic and demographic characteristics from the American Community Survey (ACS) as provided by the Boston Area Research Initiative (BARI). From the tibble, we\u0026rsquo;ll select a few variables to explore throughout. These are the median household income (med_house_income), proportion of the population that uses public transit (by_pub_trans), proportion of the population with a bachelor\u0026rsquo;s degree (bach), and then FIPS code. Additionally, there are a few missing values in our variables. We\u0026rsquo;ll fill those in with median imputation.\nThe object suffolk_county contains the boundary of each census tract and will be joined to the ACS data.\nlibrary(sf) library(tidyverse) acs \u0026lt;- select(uitk::acs_raw, fips = ct_id_10, med_house_income, by_pub_trans, bach) %\u0026gt;% mutate(fips = as.character(fips), across(.cols = c(med_house_income, by_pub_trans, bach), ~replace_na(.x, median(.x, na.rm = TRUE)))) acs_sf \u0026lt;- left_join(uitk::suffolk_county, acs, by = \"fips\")  Now that we have this object we can visualize how median household income is distributed numerically and geographically.\nacs_sf %\u0026gt;% ggplot(aes(bach)) + geom_histogram(bins = 15, fill = \"#528672\") + theme_minimal() + geom_vline(aes(xintercept = mean(bach)), lty = 3, size = .75) + labs(x = \"Median Household Income\", y = \"Frequency\", title = \"Distribution of Educational Attainment\")   acs_sf %\u0026gt;% mutate(bach_dec = ntile(bach, 10)) %\u0026gt;% ggplot(aes(fill = bach_dec)) + geom_sf(lwd = 0.2, color = \"black\") + theme_void() + scale_fill_gradient(high = \"#528672\", n.breaks = 10) + labs(title = \"Educational Attainment Deciles\")   From the graph and map we can see two things:\n The distribution of educational attainment is left skewed There appears to be clusters of low educational attainment in the north and the south  How can we check to see if there is a significant spatial relationship? We\u0026rsquo;ll need to look at the surrounding values of each observation.\nUnderstanding spatial autocorrelation Typical correlation measures explores how two continuous variables are related to each other. Does one increase when the other does? Spatial autocorrelation looks to see if a variable has any relationship in how it is distributed across a geography. With spatial autocorrelation we can ask the question \u0026ldquo;are like values near each other?\u0026rdquo; With measures of spatial auto correlation we can only know if similar values cluster near each other. Or, inversely, near values are different from each other and far ones are similar.\nThe most common measure of spatial autocorrelation is Moran\u0026rsquo;s I. Moran\u0026rsquo;s I is a number that typically ranges between -1 and 1 much like other correlation measures. Though Moran\u0026rsquo;s I can exceed either boundary in some rare cases.\nWhen I approaches 1, we can interpret Moran\u0026rsquo;s I as informing us that similar values tend to be nearby each other. When I approach -1, near values are dissimilar. We cannot determine whether the clusters are positively or negatively associated, though!\nUnderstanding neighbors If we assume that there is a spatial relationship in our data, we are taking on the belief that our data are not completely independent of each other. If nearer things are more related, then census tracts that are close to each other will have similar values. In the urban literature there is a lot of discussion of \u0026ldquo;spillover effects.\u0026quot; A spillover effect is when a change in one neighborhood affects adjacent / nearby neighborhoods. This is in essence what we are trying to evaluate.\nBecause we are concerned with what surrounding observations look like, we need to know which observations are nearby. There are a number of different ways in which neighbors can be identified. With polygon data we identify neighbors based on their contiguity. To be contiguous means to be connected or touching\u0026mdash;think of the contiguous lower 48 states.\nContiguities The two most common contiguities are based on the game of chess. Let\u0026rsquo;s take a simple chess board (code included because it\u0026rsquo;s a fun trick ðŸ˜„).\nchess_board \u0026lt;- expand.grid(x = 1:8, y = 1:8) %\u0026gt;% mutate(z = ifelse((x + y) %% 2 == 0, TRUE, FALSE)) board \u0026lt;- chess_board %\u0026gt;% ggplot(aes(x, y, fill = z)) + geom_tile() + scale_fill_manual(values = c(\"white\", \"black\")) + theme_void() + coord_fixed() + theme(legend.position = \"none\") board   In chess each piece can move in a different way. All pieces, with the exception of the knight, move either diagonally or horizontally and vertically. The most common contiguities are queen and rook contiguities. In chess, a queen can move diagonally and horizontal and vertically whereas a rook can only move horizontal and vertically.\n We extend this idea to polygons. Queen contiguities identify neighbors based on any polygon that is touching. With rook contiguities, we identify neighbors based on polygons that touch on the side. For most social science research, we only need to be concerned with queen contiguities.\n While a chess board might make intuitive sense, geographies are really wonky in real life. Let\u0026rsquo;s take a random census tract in Suffolk County and look at its queen contiguity.\n You can see that any tract that is touching, even at a corner, will be considered a neighbor to the point in question. This will be done for every polygon in our dataset. We can create a network diagram of our spatial object which can be helpful or exploring these spatial relationships visually and encourage a network based approach.\n Understanding the spatial weights Once neighbors are identified, they can then be used to calculate spatial weights. These weights will be used to identify the average local household income for surrounding census tracts. However, prior to doing so, we must know how much influence each observation will have in calculating that local estimate.\nThe typical method of calculating the spatial weights is through row standardization. In essence, each neighbor that touches our census tract will be assigned an equal weight. We do this by assigning each neighbor a value of 1 then dividing by the number of neighbors. So if we have 5 neighboring census tracts, each of them will have a spatial weight of 0.2 (1 / 5 = 0.2).\nGoing back to the chess board example, we can take the position d4 and look at the queen contiguities. There are 8 squares that immediately touch the square. Each one of these squares is considered a neighbor and given a value of 1. Then each square is divided by the total number or neighbors, 8.\n Very simply it looks like the following\n(d4_nbs \u0026lt;- rep(1, 8)) #\u0026gt; [1] 1 1 1 1 1 1 1 1 d4_nbs / length(d4_nbs) #\u0026gt; [1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125  Defining neighbor relationships Now that we have a general understanding of neighbors and weights we can go ahead and and begin to create do the work in R. For this we will use the package sfweight. Currently the package is not on CRAN and needs to be installed from GitHub. Install the package with the following: remotes::install_github(\u0026quot;josiahparry/sfweight\u0026quot;).\nThere are two functions that we will be using from this package: st_neighbors() and st_weights(). The former will take the geometry column of an sf object and create a list column containing the neighbor indexes for that observation. st_weights() will take the neighbors list and calculate a list column of weights. These functions work nicely with the tidyverse workflow so we can calculate both the neighbors and weights in one mutate function call.\nlibrary(sfweight) acs_nbs \u0026lt;- acs_sf %\u0026gt;% mutate(nb = st_neighbors(geometry), wt = st_weights(nb))  It was easy as that. We can look at the neighbor and weights columns. Notice how they are always of the same length for each row.\nNeighbors list:\npull(acs_nbs, nb)[1:5] #\u0026gt; [[1]] #\u0026gt; [1] 2 15 168 171 172 179 180 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 1 71 180 #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 45 50 92 122 #\u0026gt;  #\u0026gt; [[4]] #\u0026gt; [1] 30 84 127 135 136 138 #\u0026gt;  #\u0026gt; [[5]] #\u0026gt; [1] 34 87 100 108  Weights list:\npull(acs_nbs, wt)[1:5] #\u0026gt; [[1]] #\u0026gt; [1] 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 0.3333333 0.3333333 0.3333333 #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 0.25 0.25 0.25 0.25 #\u0026gt;  #\u0026gt; [[4]] #\u0026gt; [1] 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 #\u0026gt;  #\u0026gt; [[5]] #\u0026gt; [1] 0.25 0.25 0.25 0.25  Measuring spatial autocorrelation With all of our measures in place, we can calculate Moran\u0026rsquo;s I and check to see if there is any spatial autocorrelation.\nacs_nbs %\u0026gt;% moran_test(bach, nb, wt) #\u0026gt;  #\u0026gt; Moran I test under randomisation #\u0026gt;  #\u0026gt; data: var  #\u0026gt; weights: listw  #\u0026gt;  #\u0026gt; Moran I statistic standard deviate = 14.037, p-value \u0026lt; 2.2e-16 #\u0026gt; alternative hypothesis: greater #\u0026gt; sample estimates: #\u0026gt; Moran I statistic Expectation Variance  #\u0026gt; 0.596022182 -0.004950495 0.001833067  With this result we can tell that there is a somewhat strong spatial relationship in bachelor\u0026rsquo;s degree attainment throughout Suffolk County. To explore where these clusters are we can use the local Moran\u0026rsquo;s I. This will be covered in a forth coming post.\nMiscellaneous Resources  resource: https://spatiolog.blogspot.com/2019/01/contiguity-based-spatial-weights-matrix.html https://www.e-education.psu.edu/geog586/node/672#:~:text=The%20Moran%20scatterplot%20is%20an,same%20attribute%20at%20neighboring%20locations. https://geographicdata.science/book/notebooks/07_local_autocorrelation.html https://rspatial.org/raster/analysis/3-spauto.html http://www.dpi.inpe.br/gilberto/tutorials/software/geoda/tutorials/w8_weights.pdf  ","permalink":"/tutorials/2021-05-07-spatial-autocorrelation-in-r/","tags":["sfweight","rspatial"],"title":"Spatial Autocorrelation in R"},{"categories":[],"contents":"A paper summary in 500ish words  \u0026ldquo;political conflict over LULUs is a struggle between capital and community via the intermediary of the state.\u0026rdquo;\n \n Cartoon by Luke McGarry \nIn order to understand the rising YIMBY movement, we need to first understand its progenitor, NIMBY. There is no better paper to start at than \u0026ldquo;Rethinking NIMBY\u0026rdquo; by Robert W. Lake (1993). In this paper Lake introduces us to the common conception of NIMBY, his nuanced view of NIMBY in relation to capitalism, and the necessary political process which creates the NIMBY movement. The paper may be more aptly titled \u0026ldquo;In Defense of NIMBY.\u0026rdquo;\nNIMBY, or Not In My Back Yard, is an opposition movement to unwanted development of locally unwanted land uses (LULU). LULUs are places like landfills, roads, recovery centers and the like which have some societal benefit. NIMBY is often thought to be \u0026ldquo;selfish\u0026rdquo; and \u0026ldquo;parochial\u0026rdquo; given these proposed developments serve some good\u0026mdash;we all need roads, right? NIMBY has been a surprising success in thwarting developments in favor of \u0026ldquo;neighborhood ambiance.\u0026rdquo;\nLake challenges general antipathy towards NIMBY by addressing a problematic understanding of LULUs. Those who oppose NIMBY make the assumption that LULUs provide a societal good. But what Lake points out is that they are a \u0026ldquo;particular solution to a problem.\u0026rdquo; Acceptance of LULUs accepts the status quo and assumes that there couldn\u0026rsquo;t be a better solution to the problem.\nLake then points out that NIMBY is actually an inevitability and a sign of successful developments. Think of development like creating a product to sell. You need to have customers. In order to produce the product, you need investors who believe there will be profit. The same is true with development. Private investors build to a target consumer base to ensure future profit. Then people move in or start utilizing the developments creating a land use pattern. The development\u0026mdash;say multifamily homes with local markets and cafÃ©s\u0026mdash;will develop their own problems such as \u0026ldquo;low-density development, suburban sprawl, [and] inadequate services.\u0026rdquo;\nWe come to two challenges: 1) current consumers of the existing land use want to maintain the status quo. And 2) changes to the land use would be beneficial to investors to increase their profitability. Proposed changes to the land use are likely against NIMBY best interest but are in the best interest of private industry.\nThe propositions by private industry are not always in the interest of the existing neighborhood or in the best interest of society. Lake provides the example of hazardous waste incinerators. Hazardous waste is that, hazardous. Over a span of 15 years NIMBY collective action prevented a single incinerator from being built. Through this example we see that NIMBY can be an effective movement in challenging existing social structures.\nThus, we come to the conclusion that \u0026ldquo;local community perspective is not necessarily opposed to the societal good\u0026mdash;but nor is it necessarily synonymous with it.\u0026rdquo; Or more simply, the NIMBY movement can be helpful in challenging private industry and government to address the underlying issues that LULUs are meant to address.\n","permalink":"/blog/2021-04-25-rethinking-nimby/","tags":[],"title":"Rethinking NIMBY"},{"categories":[],"contents":"The urban data collective is an opportunity to create an publication that is solely focused on urban informatics. It is borne borne out of a desire to create an easy and non-academic resource for folks interested in or entering urban informtics. Most, if not all, resources and sites dedicated to the topic in the field are jargon heavy or overly academic. Our intention is to produce content that is engaging, consumable, and above all else, useful.\nAs it is entitled a collective, we would like to see contributions from the community. If you woud like to contribute a blog post or a tutorial, please create an issue on the github repository.\n","permalink":"/blog/2021-04-11-introducing-the-urban-collective/","tags":[],"title":"Introducing the urban data collective"},{"categories":["covid-19"],"contents":"  Studying Covid-19â€™s Effect on Community Vulnerability in Massachusetts The Covid-19 crisis has exposed the weak hidden foundations upon which our â€˜prosperousâ€™ society was built, and exacerbated visible ones. The most visible of these factors is the sudden jump in unemployment over the summer. This project aims to develop a â€˜Community Vulnerability Indexâ€™ to understand the devastating effects the health and economic crisis brought on by the Covid-19 pandemic. It aims to identify communities in Massachusetts most affected by heightened unemployment during the summer for targeted relief and developmental assistance.\n  Slide Comments     Project Cover   Data Sources and Index Development   Summer Unemployment Rates   R Code Development   R Code Development   Index Mapped   Top 10 Worst Hit   Top 10 Least Hit   Detailed Factor Analysis   Policy Recommendations     ","permalink":"/blog/2021-04-11-a-state-of-being/","tags":[],"title":"A State of Being"}]